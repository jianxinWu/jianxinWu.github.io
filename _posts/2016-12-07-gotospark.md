---
layout: post
title: Spark入门之到底什么是Spark
date: 2016-12-07
tags: 大数据   
---


### -_-序幕之DT时代介绍
 大数据时代的到来，各种数据的累积已为大数据技术做好了铺垫，各种分布式计算技术如雨后春笋兴起-大数据时代的到来，各种数据的累积已为大数据技术做好了铺垫，各种分布式计算技术如雨后春笋兴起-




> * Apache Hadoop 的MapReduce（曾经炙手可热的王者） 
> * Twitter开源的一个分布式的、容错的实时计算系统Storm（下文将提到） 
> * 接下来就是本篇博客的主角了--Spark



### 一、Spark初见

　海量的数据推进着分布式计算的前进，大数据的浪潮可谓是一波又一波，目前有三大主流分布式计算框架，分别是Hadoop、Storm、Spark。然而Hadoop作为大数据技术兴起的第一个标准，经过十多年的发展，由于设计之初并没有考虑到效率从而导致缺陷不能满足当前对实时性及迭代运算的需求。


### 二、Spark的生态系统

　Spark追求的是全栈式地解决批处理、结构化数据查询、流计算、图计算、机器学习等业务场景，因此除了核心库Spark Core之外，还有其各个子项目，包括：


> * Spark SQL  用于支持SQL查询的子项目 
> * MLlib库 支持机器学习 
> * GraphX  支持图计算
> * Spark Streaming 支持流计算（下文将详细介绍）
> * Tachyon、BlinkDB、Tungsten等实验性项目


### 三、Spark Streaming

　提到流处理框架，Storm就是一个好的话题，Storm是由Twitter公司开源的一个分布式的、容错的实时计算框架，它与Spark Streaming 功能很相似，因此值得针对二者进行对比，下面将从两方面进行比较。

　###### 1、处理数据的延迟

　Storm完成一个任务可以达到毫秒级别的延迟，而Spark Streaming则是秒级别的延迟。

　###### 2、处理的过程与结果

　在容错的方面，Spark Streaming提供了更加稳定的容错机制，由于只需要在批处理级别上进行跟踪处理（Storm是每个单独的记录都需要被跟踪），即使有一个节点发生故障，也可以保证所有数据都被处理一次。
　因此在除了对实时性要求特别高，Spark Streaming更好，因为它可以与Spark生态系统中与其他组件进行无缝交叉融合，从而在面对复杂的业务场景中，更加凸显Spark Streaming及Spark整个生态圈的重要性。


### 总结

　经过前面入门知识的概述，知道了Spark的重要性，让我们动起来，融于Spark，走上人生巅峰。最后的最后，本人也是刚入门不久的小白，写得不好，请多多指教（不喜勿喷），接下来也会根据自己的学习持续更新关于Spark的文章。